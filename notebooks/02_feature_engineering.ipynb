{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "02_feature_engineering.ipynb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title-cell"
   },
   "source": [
    "# 02 - Feature Engineering | SweetReturns Golden City\n",
    "\n",
    "**Notebook 2 of 5** in the SweetReturns data pipeline.\n",
    "\n",
    "This notebook reads the cleaned parquet from notebook 01 and computes all technical/quantitative features:\n",
    "- Drawdown features\n",
    "- Volatility features\n",
    "- Volume features\n",
    "- Momentum indicators (RSI, MACD, Bollinger Bands, OBV, ATR, Stochastic, ROC, MFI)\n",
    "- Hurst exponent\n",
    "- SPY benchmark + relative features\n",
    "- Graph/network features (correlation matrix + centrality)\n",
    "- HMM regime detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy scipy hmmlearn scikit-learn networkx python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_parquet(\"stock_data_clean.parquet\")\n",
    "print(f\"Loaded: {df.shape}, Tickers: {df['ticker'].nunique()}\")\n",
    "df = df.sort_values([\"ticker\", \"Date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drawdown-features"
   },
   "outputs": [],
   "source": [
    "# Compute drawdown from all-time high (per ticker)\n",
    "def compute_drawdown_features(group):\n",
    "    close = group[\"Close\"]\n",
    "    expanding_max = close.expanding().max()\n",
    "    group[\"drawdown_pct\"] = (close - expanding_max) / expanding_max\n",
    "    group[\"drawdown_percentile\"] = group[\"drawdown_pct\"].rank(pct=True)\n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"ticker\", group_keys=False).apply(compute_drawdown_features)\n",
    "print(f\"Drawdown stats:\\n{df['drawdown_pct'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "volatility-features"
   },
   "outputs": [],
   "source": [
    "def compute_volatility_features(group):\n",
    "    ret = group[\"daily_return\"]\n",
    "    group[\"realized_vol_20d\"] = ret.rolling(20).std() * np.sqrt(252)\n",
    "    group[\"realized_vol_60d\"] = ret.rolling(60).std() * np.sqrt(252)\n",
    "    \n",
    "    # Vol percentile: where current vol sits vs its own 1-year history\n",
    "    vol = group[\"realized_vol_20d\"]\n",
    "    group[\"vol_percentile\"] = vol.rolling(252, min_periods=60).apply(\n",
    "        lambda x: stats.percentileofscore(x[:-1], x.iloc[-1]) / 100 if len(x) > 1 else 0.5,\n",
    "        raw=False\n",
    "    )\n",
    "    \n",
    "    # Vol term structure\n",
    "    group[\"vol_term_structure\"] = group[\"realized_vol_20d\"] / group[\"realized_vol_60d\"].replace(0, np.nan)\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"ticker\", group_keys=False).apply(compute_volatility_features)\n",
    "print(f\"Vol stats:\\n{df[['realized_vol_20d', 'vol_percentile']].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "volume-features"
   },
   "outputs": [],
   "source": [
    "def compute_volume_features(group):\n",
    "    vol = group[\"Volume\"].astype(float)\n",
    "    group[\"volume_percentile\"] = vol.rolling(252, min_periods=60).apply(\n",
    "        lambda x: stats.percentileofscore(x[:-1], x.iloc[-1]) / 100 if len(x) > 1 else 0.5,\n",
    "        raw=False\n",
    "    )\n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"ticker\", group_keys=False).apply(compute_volume_features)\n",
    "print(f\"Volume percentile stats:\\n{df['volume_percentile'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "momentum-features"
   },
   "outputs": [],
   "source": [
    "def compute_momentum_features(group):\n",
    "    close = group[\"Close\"]\n",
    "    high = group[\"High\"]\n",
    "    low = group[\"Low\"]\n",
    "    volume = group[\"Volume\"].astype(float)\n",
    "    ret = group[\"daily_return\"]\n",
    "    \n",
    "    # RSI 14\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = (-delta).clip(lower=0)\n",
    "    avg_gain = gain.rolling(14).mean()\n",
    "    avg_loss = loss.rolling(14).mean()\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    group[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD (12, 26, 9)\n",
    "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
    "    group[\"macd_line\"] = ema12 - ema26\n",
    "    group[\"macd_signal\"] = group[\"macd_line\"].ewm(span=9, adjust=False).mean()\n",
    "    group[\"macd_histogram\"] = group[\"macd_line\"] - group[\"macd_signal\"]\n",
    "    \n",
    "    # Bollinger Bands (20, 2)\n",
    "    bb_mid = close.rolling(20).mean()\n",
    "    bb_std = close.rolling(20).std()\n",
    "    group[\"bb_upper\"] = bb_mid + 2 * bb_std\n",
    "    group[\"bb_lower\"] = bb_mid - 2 * bb_std\n",
    "    group[\"bb_width\"] = (group[\"bb_upper\"] - group[\"bb_lower\"]) / bb_mid\n",
    "    group[\"bb_pctb\"] = (close - group[\"bb_lower\"]) / (group[\"bb_upper\"] - group[\"bb_lower\"]).replace(0, np.nan)\n",
    "    \n",
    "    # OBV\n",
    "    obv = (np.sign(close.diff()) * volume).fillna(0).cumsum()\n",
    "    group[\"obv\"] = obv\n",
    "    # OBV slope (20-day linear regression slope)\n",
    "    group[\"obv_slope_20d\"] = obv.rolling(20).apply(\n",
    "        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 20 else 0, raw=False\n",
    "    )\n",
    "    \n",
    "    # ATR 14\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    group[\"atr_14\"] = tr.rolling(14).mean()\n",
    "    group[\"atr_percentile\"] = group[\"atr_14\"].rolling(252, min_periods=60).apply(\n",
    "        lambda x: stats.percentileofscore(x[:-1], x.iloc[-1]) / 100 if len(x) > 1 else 0.5,\n",
    "        raw=False\n",
    "    )\n",
    "    \n",
    "    # Stochastic Oscillator (14, 3)\n",
    "    low14 = low.rolling(14).min()\n",
    "    high14 = high.rolling(14).max()\n",
    "    group[\"stoch_k\"] = 100 * (close - low14) / (high14 - low14).replace(0, np.nan)\n",
    "    group[\"stoch_d\"] = group[\"stoch_k\"].rolling(3).mean()\n",
    "    \n",
    "    # Rate of Change\n",
    "    group[\"roc_5\"] = close.pct_change(5)\n",
    "    group[\"roc_20\"] = close.pct_change(20)\n",
    "    group[\"roc_60\"] = close.pct_change(60)\n",
    "    \n",
    "    # MFI 14\n",
    "    typical_price = (high + low + close) / 3\n",
    "    mf = typical_price * volume\n",
    "    pos_mf = mf.where(typical_price > typical_price.shift(1), 0).rolling(14).sum()\n",
    "    neg_mf = mf.where(typical_price <= typical_price.shift(1), 0).rolling(14).sum()\n",
    "    group[\"mfi_14\"] = 100 - (100 / (1 + pos_mf / neg_mf.replace(0, np.nan)))\n",
    "    \n",
    "    # Z-scores\n",
    "    group[\"zscore_20d\"] = (ret - ret.rolling(20).mean()) / ret.rolling(20).std().replace(0, np.nan)\n",
    "    group[\"zscore_60d\"] = (ret - ret.rolling(60).mean()) / ret.rolling(60).std().replace(0, np.nan)\n",
    "    \n",
    "    # Autocorrelation (60-day rolling, lag 1)\n",
    "    group[\"autocorrelation_lag1\"] = ret.rolling(60).apply(\n",
    "        lambda x: x.autocorr(lag=1) if len(x) == 60 else 0, raw=False\n",
    "    )\n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"ticker\", group_keys=False).apply(compute_momentum_features)\n",
    "print(f\"Feature columns: {len(df.columns)}\")\n",
    "print(df[[\"rsi_14\", \"macd_histogram\", \"bb_pctb\", \"stoch_k\", \"mfi_14\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hurst-exponent"
   },
   "outputs": [],
   "source": [
    "def compute_hurst(series, max_lag=20):\n",
    "    \"\"\"Compute Hurst exponent via R/S analysis.\"\"\"\n",
    "    lags = range(2, max_lag)\n",
    "    tau = []\n",
    "    for lag in lags:\n",
    "        chunks = [series[i:i+lag] for i in range(0, len(series) - lag, lag)]\n",
    "        rs_values = []\n",
    "        for chunk in chunks:\n",
    "            if len(chunk) < 2:\n",
    "                continue\n",
    "            mean_val = np.mean(chunk)\n",
    "            cumdev = np.cumsum(chunk - mean_val)\n",
    "            R = np.max(cumdev) - np.min(cumdev)\n",
    "            S = np.std(chunk, ddof=1)\n",
    "            if S > 0:\n",
    "                rs_values.append(R / S)\n",
    "        if rs_values:\n",
    "            tau.append(np.mean(rs_values))\n",
    "        else:\n",
    "            tau.append(np.nan)\n",
    "    \n",
    "    valid = [(l, t) for l, t in zip(lags, tau) if not np.isnan(t) and t > 0]\n",
    "    if len(valid) < 3:\n",
    "        return 0.5\n",
    "    log_lags = np.log([v[0] for v in valid])\n",
    "    log_tau = np.log([v[1] for v in valid])\n",
    "    slope, _, _, _, _ = stats.linregress(log_lags, log_tau)\n",
    "    return np.clip(slope, 0, 1)\n",
    "\n",
    "def compute_hurst_rolling(group):\n",
    "    ret = group[\"daily_return\"].values\n",
    "    hurst = np.full(len(ret), np.nan)\n",
    "    for i in range(252, len(ret)):\n",
    "        hurst[i] = compute_hurst(ret[i-252:i])\n",
    "    group[\"hurst_exponent\"] = hurst\n",
    "    return group\n",
    "\n",
    "df = df.groupby(\"ticker\", group_keys=False).apply(compute_hurst_rolling)\n",
    "print(f\"Hurst exponent stats:\\n{df['hurst_exponent'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spy-benchmark"
   },
   "outputs": [],
   "source": [
    "# Download SPY data using yfinance as fallback, or use from dataset if present\n",
    "spy_data = df[df[\"ticker\"] == \"SPY\"][[\"Date\", \"Close\", \"daily_return\"]].copy()\n",
    "if len(spy_data) == 0:\n",
    "    print(\"SPY not in dataset, using mean market return as proxy\")\n",
    "    market_return = df.groupby(\"Date\")[\"daily_return\"].mean().reset_index()\n",
    "    market_return.columns = [\"Date\", \"spy_return\"]\n",
    "else:\n",
    "    spy_data = spy_data.rename(columns={\"daily_return\": \"spy_return\", \"Close\": \"spy_close\"})\n",
    "    market_return = spy_data[[\"Date\", \"spy_return\"]]\n",
    "\n",
    "df = df.merge(market_return, on=\"Date\", how=\"left\")\n",
    "\n",
    "# 20-day relative return vs SPY\n",
    "df[\"stock_return_20d\"] = df.groupby(\"ticker\")[\"Close\"].pct_change(20)\n",
    "df[\"spy_return_20d\"] = df[\"spy_return\"].rolling(20).sum()  # approximate\n",
    "df[\"relative_return_vs_spy\"] = df[\"stock_return_20d\"] - df[\"spy_return_20d\"]\n",
    "\n",
    "# Beta (252-day rolling)\n",
    "# Simplified: we'll compute sector-relative z-score instead\n",
    "def compute_sector_zscore(group):\n",
    "    ret20 = group[\"stock_return_20d\"]\n",
    "    group[\"sector_zscore\"] = (ret20 - ret20.mean()) / ret20.std().replace(0, np.nan)\n",
    "    return group\n",
    "\n",
    "df = df.groupby([\"sector\", \"Date\"], group_keys=False).apply(compute_sector_zscore)\n",
    "print(f\"Relative features computed. Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graph-features"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "try:\n",
    "    from community import community_louvain\n",
    "    HAS_LOUVAIN = True\n",
    "except ImportError:\n",
    "    HAS_LOUVAIN = False\n",
    "    print(\"python-louvain not available, skipping community detection\")\n",
    "\n",
    "# Use most recent 252 days for correlation\n",
    "latest_date = df[\"Date\"].max()\n",
    "recent = df[df[\"Date\"] > latest_date - pd.Timedelta(days=365)]\n",
    "pivot = recent.pivot_table(index=\"Date\", columns=\"ticker\", values=\"daily_return\")\n",
    "corr_matrix = pivot.corr()\n",
    "\n",
    "# Build graph from high correlations\n",
    "G = nx.Graph()\n",
    "tickers = corr_matrix.columns.tolist()\n",
    "G.add_nodes_from(tickers)\n",
    "for i, t1 in enumerate(tickers):\n",
    "    for j, t2 in enumerate(tickers):\n",
    "        if j > i:\n",
    "            w = corr_matrix.iloc[i, j]\n",
    "            if abs(w) > 0.5:\n",
    "                G.add_edge(t1, t2, weight=w)\n",
    "\n",
    "degree_cent = nx.degree_centrality(G)\n",
    "betweenness_cent = nx.betweenness_centrality(G)\n",
    "try:\n",
    "    eigenvector_cent = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "except:\n",
    "    eigenvector_cent = {t: 0 for t in tickers}\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Community detection\n",
    "if HAS_LOUVAIN:\n",
    "    communities = community_louvain.best_partition(G)\n",
    "else:\n",
    "    communities = {t: 0 for t in tickers}\n",
    "\n",
    "# Map back to DataFrame\n",
    "graph_features = pd.DataFrame({\n",
    "    \"ticker\": tickers,\n",
    "    \"degree_centrality\": [degree_cent.get(t, 0) for t in tickers],\n",
    "    \"betweenness_centrality\": [betweenness_cent.get(t, 0) for t in tickers],\n",
    "    \"eigenvector_centrality\": [eigenvector_cent.get(t, 0) for t in tickers],\n",
    "    \"pagerank\": [pagerank.get(t, 0) for t in tickers],\n",
    "    \"community_id\": [communities.get(t, 0) for t in tickers],\n",
    "})\n",
    "graph_features[\"community_size\"] = graph_features[\"community_id\"].map(\n",
    "    graph_features.groupby(\"community_id\")[\"ticker\"].transform(\"count\")\n",
    ")\n",
    "\n",
    "df = df.merge(graph_features, on=\"ticker\", how=\"left\")\n",
    "print(f\"Graph features added. Shape: {df.shape}\")\n",
    "print(f\"Communities found: {graph_features['community_id'].nunique()}\")\n",
    "print(f\"Edges in graph: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmm-regime"
   },
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# Prepare SPY features for regime detection\n",
    "spy_df = df[df[\"ticker\"] == df[\"ticker\"].iloc[0]][[\"Date\", \"spy_return\"]].drop_duplicates(\"Date\").sort_values(\"Date\").dropna()\n",
    "if len(spy_df) < 100:\n",
    "    spy_df = df.groupby(\"Date\")[\"daily_return\"].mean().reset_index()\n",
    "    spy_df.columns = [\"Date\", \"spy_return\"]\n",
    "    spy_df = spy_df.sort_values(\"Date\").dropna()\n",
    "\n",
    "spy_vol = spy_df[\"spy_return\"].rolling(20).std() * np.sqrt(252)\n",
    "hmm_features = pd.DataFrame({\n",
    "    \"return\": spy_df[\"spy_return\"].values,\n",
    "    \"vol\": spy_vol.values,\n",
    "}).dropna()\n",
    "\n",
    "# Fit 4-state HMM\n",
    "model = GaussianHMM(n_components=4, covariance_type=\"full\", n_iter=200, random_state=42)\n",
    "model.fit(hmm_features.values)\n",
    "regimes = model.predict(hmm_features.values)\n",
    "\n",
    "# Label regimes by mean return and vol\n",
    "regime_stats = pd.DataFrame(hmm_features.values, columns=[\"return\", \"vol\"])\n",
    "regime_stats[\"regime\"] = regimes\n",
    "regime_summary = regime_stats.groupby(\"regime\").agg({\"return\": \"mean\", \"vol\": \"mean\"})\n",
    "print(\"Regime summary:\\n\", regime_summary)\n",
    "\n",
    "# Map to semantic labels\n",
    "regime_labels = {}\n",
    "for r in regime_summary.index:\n",
    "    bull = regime_summary.loc[r, \"return\"] > 0\n",
    "    quiet = regime_summary.loc[r, \"vol\"] < regime_summary[\"vol\"].median()\n",
    "    if bull and quiet: regime_labels[r] = \"bull_quiet\"\n",
    "    elif bull and not quiet: regime_labels[r] = \"bull_volatile\"\n",
    "    elif not bull and quiet: regime_labels[r] = \"bear_quiet\"\n",
    "    else: regime_labels[r] = \"bear_volatile\"\n",
    "\n",
    "# Merge regime back to dates\n",
    "aligned_dates = spy_df[\"Date\"].iloc[19:].reset_index(drop=True)  # offset by rolling window\n",
    "regime_df = pd.DataFrame({\"Date\": aligned_dates.values[:len(regimes)], \"regime_label\": [regime_labels[r] for r in regimes]})\n",
    "df = df.merge(regime_df, on=\"Date\", how=\"left\")\n",
    "df[\"regime_label\"] = df[\"regime_label\"].fillna(\"unknown\")\n",
    "print(f\"\\nRegime distribution:\\n{df['regime_label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-features"
   },
   "outputs": [],
   "source": [
    "# Save feature-rich dataset\n",
    "df.to_parquet(\"stock_features.parquet\", index=False)\n",
    "corr_matrix.to_parquet(\"correlation_matrix.parquet\")\n",
    "graph_features.to_parquet(\"graph_features.parquet\")\n",
    "print(f\"Saved stock_features.parquet: {df.shape} ({len(df.columns)} columns)\")\n",
    "print(f\"Saved correlation_matrix.parquet: {corr_matrix.shape}\")\n",
    "print(f\"\\nAll columns:\\n{list(df.columns)}\")\n",
    "print(f\"\\nReady for 03_golden_tickets.ipynb!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-cell"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook computed the following feature categories:\n",
    "\n",
    "| Category | Features |\n",
    "|----------|----------|\n",
    "| **Drawdown** | `drawdown_pct`, `drawdown_percentile` |\n",
    "| **Volatility** | `realized_vol_20d`, `realized_vol_60d`, `vol_percentile`, `vol_term_structure` |\n",
    "| **Volume** | `volume_percentile` |\n",
    "| **RSI** | `rsi_14` |\n",
    "| **MACD** | `macd_line`, `macd_signal`, `macd_histogram` |\n",
    "| **Bollinger Bands** | `bb_upper`, `bb_lower`, `bb_width`, `bb_pctb` |\n",
    "| **OBV** | `obv`, `obv_slope_20d` |\n",
    "| **ATR** | `atr_14`, `atr_percentile` |\n",
    "| **Stochastic** | `stoch_k`, `stoch_d` |\n",
    "| **Rate of Change** | `roc_5`, `roc_20`, `roc_60` |\n",
    "| **MFI** | `mfi_14` |\n",
    "| **Z-scores** | `zscore_20d`, `zscore_60d` |\n",
    "| **Autocorrelation** | `autocorrelation_lag1` |\n",
    "| **Hurst** | `hurst_exponent` |\n",
    "| **Relative** | `spy_return`, `stock_return_20d`, `spy_return_20d`, `relative_return_vs_spy`, `sector_zscore` |\n",
    "| **Graph/Network** | `degree_centrality`, `betweenness_centrality`, `eigenvector_centrality`, `pagerank`, `community_id`, `community_size` |\n",
    "| **Regime** | `regime_label` |\n",
    "\n",
    "### Output files\n",
    "- `stock_features.parquet` - Full dataset with all features\n",
    "- `correlation_matrix.parquet` - Ticker-to-ticker correlation matrix\n",
    "- `graph_features.parquet` - Network centrality features per ticker\n",
    "\n",
    "### Next step\n",
    "Proceed to **03_golden_tickets.ipynb** for signal generation and stock selection."
   ]
  }
 ]
}